{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2025 Assignment 3\n",
    "### Instructor: Brandon Haworth\n",
    "#### Notebook Credit: George Tzanetakis\n",
    "Jupyter Notebooks you encounter during the course were largely developed by Prof. Tzanetakis from a previous iteration of this course. I've since changed/developed them where necessary for my own iterations of CSC 421.\n",
    "\n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 12 - Quantifying Uncertainty** and **Chapter 13 Probabilistic Reasoning** from the book *Artificial Intelligence: A Modern Approach.*  \n",
    "\n",
    "The assignment structure is as follows:\n",
    "\n",
    "1. [6 Marks]: Non-transitive dice war (Basic)   \n",
    "2. [6 Marks]: Text Categorization setup (Basic) \n",
    "3. [8 Marks]: Text Classification (Expected) \n",
    "4. [6 Marks]: Specifying a Bayesian Network (Basic) \n",
    "5. [6 Marks]: Inference on a Bayesian Network (Expected) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  Non-transitive dice war - 6 Marks\n",
    "\n",
    "In this question, we will explore generating samples of discrete random variables and the fascinating concept of non-transitive dice. A dice war is a game in which two dice with \n",
    "different probability distributions are randomly sampled, the corresponding \n",
    "samples are compared, and the one with the highest number is counted as a \"win\". If a dice $A$ wins on average more than half of the time against another dice $B$ we say that dice $A$ beats dice $B$. \n",
    "\n",
    "For example let's consider a standard dice $A$ with probability distribution: $P(A) = <\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}>$ and a dice $B$ that has 3 faces with the number $4$ and three faces with the number $5$ so that $P(B) = <0, 0, 0, \\frac{3}{6}, \\frac{3}{6}, 0>$.  \n",
    "\n",
    "In the cell below, code is provided for defining random variables by providing an array of values and an array of corresponding probabilities. Complete the function *dice_war* based on the code and documentation provided in the cell below. Show that the dice $B$ described above wins on average the war against dice $A$. \n",
    "\n",
    "Now consider the following three dice/random variables $Red, Green, Blue$. They all have six faces \n",
    "but different values. The corresponding values are: \n",
    "\n",
    "1. $[2,2,4,4,9,9]$ for $Red$\n",
    "2. $[1,1,6,6,8,8]$ for $Green$ \n",
    "3. $[3,3,5,5,7,7]$ for $Blue$\n",
    "\n",
    "<img src=\"IntransitiveDice.png\" width=\"25%\">\n",
    "\n",
    "Using the dice war function you wrote show the counter-intuitive result that the $Red$ die beats the $Green$ die, the $Green$ die beats the $Blue$ die, but the $Blue$ die beats the $Red$ die. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DieB beats DieA with probability 0.76\n",
      "Red beats Green with probability 0.552\n",
      "Green beats Blue with probability 0.554\n",
      "Blue beats Red with probability 0.555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class Random_Variable: \n",
    "    \n",
    "    def __init__(self, name, values, probability_distribution): \n",
    "        self.name = name \n",
    "        self.values = values \n",
    "        self.probability_distribution = probability_distribution\n",
    "        if all(type(item) is np.int64 for item in values):\n",
    "            self.type = 'numeric'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (values, probability_distribution))\n",
    "        elif all(type(item) is str for item in values): \n",
    "            self.type = 'symbolic'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (np.arange(len(values)), probability_distribution))\n",
    "            self.symbolic_values = values \n",
    "        else: \n",
    "            self.type = 'undefined'\n",
    "            \n",
    "    def sample(self,size): \n",
    "        if (self.type =='numeric'):\n",
    "            return self.rv.rvs(size=size)\n",
    "        elif (self.type == 'symbolic'): \n",
    "            numeric_samples = self.rv.rvs(size=size)\n",
    "            mapped_samples = [self.values[x] for x in numeric_samples]\n",
    "            return mapped_samples\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "def dice_war(A,B, num_samples = 1000, output=True):\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    #Generate samples for the dice\n",
    "    A_samples = A.sample(size=num_samples)\n",
    "    B_samples = B.sample(size=num_samples)\n",
    "\n",
    "    #Sum up the number of times that A wins\n",
    "    A_wins = np.sum(A_samples > B_samples)\n",
    "\n",
    "    #Calculate the probablity of A winning\n",
    "    prob = A_wins / num_samples\n",
    "    \n",
    "    res = prob > 0.5 \n",
    "    \n",
    "    if output: \n",
    "        if res:\n",
    "            print('{} beats {} with probability {}'.format(A.get_name(),\n",
    "                                                           B.get_name(),\n",
    "                                                           prob))\n",
    "        else:\n",
    "            print('{} beats {} with probability {:.2f}'.format(B.get_name(),\n",
    "                                                               A.get_name(),\n",
    "                                                               1.0-prob))\n",
    "    return (res, prob)\n",
    "\n",
    "\n",
    "# Example: Create two dice from the example above A and B\n",
    "values = np.arange(1,7,dtype=np.int64)\n",
    "probabilities_A = np.array([1/6., 1/6., 1/6., 1/6., 1/6., 1/6.])\n",
    "probabilities_B = np.array([0/6., 0/6., 0/6., 3/6., 3/6., 0/6.])\n",
    "\n",
    "dieA = Random_Variable('DieA', values, probabilities_A)\n",
    "dieB = Random_Variable('DieB', values, probabilities_B)\n",
    "\n",
    "(res, prob)=dice_war(dieA,dieB)\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE \n",
    "# Add code here to show the non-transitive nature of Red, Green, and Blue dice \n",
    "# Create three dice Red Green Blue\n",
    "\n",
    "#Set the new possible values to 1-9\n",
    "values_colours = np.arange(1,10,dtype=np.int64)\n",
    "\n",
    "#Set the probabilities of each die\n",
    "probabilities_Red = np.array([0, 2/6., 0, 2/6., 0, 0, 0, 0, 2/6.])\n",
    "probabilities_Green = np.array([2/6., 0, 0, 0, 0, 2/6., 0, 2/6., 0])\n",
    "probabilities_Blue = np.array([0, 0, 2/6., 0, 2/6., 0, 2/6., 0, 0])\n",
    "\n",
    "#Create the die\n",
    "dieRed = Random_Variable('Red', values_colours, probabilities_Red)\n",
    "dieGreen = Random_Variable('Green', values_colours, probabilities_Green)\n",
    "dieBlue = Random_Variable('Blue', values_colours, probabilities_Blue)\n",
    "\n",
    "#Run the dice wars and print output\n",
    "(res, prob)=dice_war(dieRed,dieGreen)\n",
    "(res, prob)=dice_war(dieGreen,dieBlue)\n",
    "(res, prob)=dice_war(dieBlue,dieRed)\n",
    "\n",
    "# Your output from this cell should look something like this, NOTE that the numbers will differ because of sampling, but the outcome should be the same\n",
    "# DieB beats DieA with probability 0.75\n",
    "# Red beats Green with probability 0.545\n",
    "# Green beats Blue with probability 0.551\n",
    "# Blue beats Red with probability 0.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Basic) - Text categorization setup - 6 Marks\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of categories on the basis of the text it contains. Naive Bayes models are often used for this task. In these models, the query variable is the document category, and the effect variables are the presence/absence of each word in the language; the assumption is that words occur independently in documents within a given category (conditional independence), with frequencies determined by the document category. Download the following file: http://www.cs.cornell.edu/People/pabo/movie-review-data/review_polarity.tar.gz containing a dataset that has been used for text mining consisting of movie reviews classified into negative and positive. You will see that there are two folders for the positive and negative categories, and they each contain multiple text files with the reviews. You can find more information about the dataset at: http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "Our goal will be to build a simple Bernoulli Naive Bayes classifier for this dataset. More complicated approaches using term frequency and inverse document frequency weighting and many more words are possible, but the basic concepts are the same. The goal is to understand the whole process, so DO NOT use existing machine learning packages but rather build the classifier from scratch.\n",
    "\n",
    "Our feature vector representation for each text file will be simply a binary vector (hence Bernoulli) that shows which of the following words are present in the text file: awful bad boring dull effective enjoyable great hilarious adoxography. For example, the text file cv996_11592.txt would be represented as (0, 0, 0, 0, 1, 0, 1, 0, 0) because it contains Effective and Great but none of the other words. I've added a word here that does not exist in the dataset. Because of this, you need to ensure you have correctly implemented Laplace smoothing. For Benoulli Bayes this is, $\\frac{N_{wC} + \\alpha}{N_{C} + |V|\\alpha}$, where $N_{wC}$ is the number of examples with this word in this class, $N_{C}$ is the total number of examples in this class, $V$ is the vocabulary so $|V|$ is the size of the vocabulary, and $\\alpha$ is the smoothing value, for Laplace Smoothing this is $\\alpha = 1$.\n",
    "\n",
    "Your job is to write code that parses the text files and calculates probabilities for each dictionary word given the review polarity:\n",
    "\n",
    "1. Write a function that can generate the feature vector of a single file given a path to that file. As a test you should be able to generate the example above for the file review_polarity/txt_sentoken/pos/cv996_11592.txt\n",
    "   ```python\n",
    "   def get_feature_vector(path):\n",
    "   ```\n",
    "   *HINT: for reading files in all at once using python:*\n",
    "    ```python\n",
    "   f = open(path, \"r\")\n",
    "   contents = f.read()\n",
    "   ```\n",
    "2. Write a function that can generate the conditional probabilities of each word given in each class, given the directory of the data for a class. You must implement Laplace Smoothing for Bernoulli Naive Bayes correctly.\n",
    "   ```python\n",
    "   def word_probabilities(directory):\n",
    "   ```\n",
    "\n",
    "NOTE: os.scandir(directory), filename.is_file(), open(filename.path, \"r\") are useful here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example pos/cv996_11592.txt:  [0 0 0 0 1 0 1 0 0]\n",
      "Negative vocabulary probabilities:  [0.12190287 0.54112983 0.17443013 0.10109019 0.08622398 0.05450942\n",
      " 0.31813677 0.05946482 0.00099108]\n",
      "[0.03468781 0.27849356 0.05450942 0.02576809 0.15361744 0.09613479\n",
      " 0.48166501 0.13181368 0.00099108]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE GOES HERE \n",
    "vocab = [\"awful\", \"bad\", \"boring\", \"dull\", \"effective\", \"enjoyable\", \"great\", \"hilarious\", \"adoxography\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def get_feature_vector(path):\n",
    "    #Read in the file contents\n",
    "    f = open(path, \"r\")\n",
    "    contents = f.read()\n",
    "    \n",
    "    #Convert all characters to lowercase\n",
    "    contents = contents.lower()\n",
    "\n",
    "    #Create an empty vector of 0's (meaning each word hasn't been seen)\n",
    "    vec = np.zeros(vocab_size, dtype=int)\n",
    "\n",
    "    #Iterate through contents to determine if the words in the vocab are contained within\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in contents:\n",
    "            vec[i] = 1\n",
    "\n",
    "    return vec\n",
    "\n",
    "def word_probabilities(direc):\n",
    "    #Set the Laplace smoothing value\n",
    "    a = 1\n",
    "\n",
    "    #Initialize a list to hold the vectors\n",
    "    vecs = []\n",
    "    \n",
    "    #Iterate through all files in the given directory and add the vectors to the array\n",
    "    for filename in os.scandir(direc):\n",
    "        vec = get_feature_vector(filename.path)\n",
    "        if vec.size > 0:\n",
    "            vecs.append(vec)\n",
    "\n",
    "    #Convert the list of vectors into a matrix\n",
    "    matrix = np.array(vecs)\n",
    "    \n",
    "    #NC is the number of files in class\n",
    "    NC = matrix.shape[0]\n",
    "    \n",
    "    #NwC is the number of files containing each word\n",
    "    #Summing the 0 axis gives the number of 1's for each word (times seen)\n",
    "    NwC = np.sum(matrix, axis=0)\n",
    "    \n",
    "    #Set the numerator and denominator for our formula\n",
    "    num = NwC + a\n",
    "    denom = NC + (vocab_size * a)\n",
    "    \n",
    "    #Calculate our formula\n",
    "    result = num / denom\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test cases\n",
    "example_vec = get_feature_vector('review_polarity/txt_sentoken/pos/cv996_11592.txt')\n",
    "neg_probs = word_probabilities('review_polarity/txt_sentoken/neg')\n",
    "pos_probs = word_probabilities('review_polarity/txt_sentoken/pos')\n",
    "\n",
    "print(\"Example pos/cv996_11592.txt: \", example_vec)\n",
    "print(\"Negative vocabulary probabilities: \", neg_probs)\n",
    "print(pos_probs)\n",
    "\n",
    "# Expected output - note that numbers may vary depending on how you parse but it should not be by much\n",
    "# [0 0 0 0 1 0 1 0 0]\n",
    "# [0.12190287 0.54112983 0.17443013 0.10109019 0.08622398 0.05450942\n",
    "#  0.31813677 0.05946482 0.00099108]\n",
    "# [0.03468781 0.27849356 0.05450942 0.02576809 0.15361744 0.09613479\n",
    "#  0.48166501 0.13181368 0.00099108]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 (EXPECTED) - Text classification  - 8 Marks\n",
    "\n",
    "Write Python code for classifying a particular test instance (in our case, a movie review) following a Bernoulli Naive Bayes approach, i.e. you just model the presence/absence of each word. NOTE: because we are not counting word occurences we are simply using a binary presence feature vector (the word appears or does not apepar in a document), we can use the Bernoulli approach. Your code should calculate the likelihood that the review is positive, given the corresponding conditional probabilities for each dictionary word, as well as the likelihood that the review is negative, given the corresponding conditional probabilities for each dictionary word. Check that your code works by providing a few example cases of prediction. Your code should be written from \"scratch\" and only use NumPy but **NOT** machine learning libraries like scikit-learn, tensorflow or pytorch. \n",
    "\n",
    "You should write three functions:\n",
    "\n",
    "1. A function that computes the likelihood given the path to a file and the conditional probabilities for a particular class. In Bernoulli Bayes we care about the exclusion terms, i.e. the likelihood is $ p(\\mathbf{x} | C_k) = \\prod_{i = 0}^{n}  p_{k_i}^{x_i}(1 - p_{k_i})^{1 - x_i}$ where $\\mathbf{x}$ is the feature vector for a particular file, $C_k$ is the class k (for us there are two pos and neg), $n$ is the number of vocabulary items, $p_{k_i}$ is the porbability of the vocabulary word $i$ appearing in class $k$, $x_i$ is the $i'th$ entry in the feature vector $\\mathbf{x}$. Note how $x_i$ is essentialy an indicator function so that the likelihood accounts for the entire vocabulary even if the word is not present. In that case we get the exclusion probability $1 - p_{k_i}$. Bernoulli Bayes is interesting in that way!\n",
    "   ```python\n",
    "   def likelihood(path, probs): \n",
    "   ```\n",
    "2. A function that computes the class priors given two directories, and returns the results as a list [$p(C_1)$, $p(C_2)$]. Yes we are aware this is a nicely balanced datset where the priors are 50/50, however you can not assume that!\n",
    "   ```python\n",
    "   def class_priors(class_dir_1, class_dir_2): \n",
    "   ```\n",
    "3. A function that returns the class prediction as a string 'positive' or 'negative' depending on the bernouli bayes outcome, NOTE: since we only care about argmax, and the denominator is the same for both classes, you can ignore the denominator. That is your predict function should be $\\underset{k \\in \\{1, \\ldots, K\\}}{\\operatorname{argmax}} p(C_k)p(\\mathbf{x} | C_k)$, note for us $k$ is $2$ because we just have a positive and a negative class.\n",
    "   ```python\n",
    "   def predict(path, neg_probs, pos_probs, neg_prior, pos_prior):\n",
    "   ```\n",
    "4. A function that returns the accuracy of the classifier. Note we would normally, at the very least, split the data into training and test sets. However, here I'd like you write a function that takes two directories, classifies all the files and computes the overall accuracy. Accuracy is $\\frac{TP + TN}{TP + TN + FP + FN}$ where $TP, TN, FP, FN$ are true positives, true ngatives, false positives, and false negatives respectively, i.e., correct classiffications over all classifications.\n",
    "   ```python\n",
    "   def accuracy(neg_dir, pos_dir): \n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_polarity/txt_sentoken/pos/cv996_11592.txt classified as positive\n",
      "review_polarity/txt_sentoken/pos/cv000_29590.txt classified as negative\n",
      "review_polarity/txt_sentoken/neg/cv001_19502.txt classified as positive\n",
      "review_polarity/txt_sentoken/neg/cv000_29416.txt classified as negative\n",
      "Overall Accuracy:  67.05 %\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "def likelihood(path, probs):\n",
    "    # Gets likelihood (P(x|Ck) for a document given a class, using Bernoulli Naive Bayes\n",
    "    x = get_feature_vector(path)\n",
    "\n",
    "    # probs is already P(wi|Ck) for each entry i, 1.0-probs is P(~wi|Ck) (probability word is absent)\n",
    "    prob_absent = 1.0-probs\n",
    "\n",
    "    # Start with absent probabilities, mask with probabilities for present words when x=1 (feature vector has a 1)\n",
    "    final_prob = prob_absent\n",
    "    final_prob[x==1] = probs[x==1]\n",
    "    \n",
    "    # Final Likelihood is the product of all probabilities\n",
    "    result = np.prod(final_prob)\n",
    "    return result\n",
    "\n",
    "def class_priors(class_dir_1, class_dir_2):\n",
    "    # Count number of files in directories 1 and 2\n",
    "    class_1_count = len(os.listdir(class_dir_1))\n",
    "    class_2_count = len(os.listdir(class_dir_2))\n",
    "    total = class_1_count + class_2_count\n",
    "\n",
    "    # Set the priors [p(C1), p(C2)]\n",
    "    prior_1 = class_1_count / total\n",
    "    prior_2 = class_2_count / total\n",
    "    \n",
    "    return [prior_1, prior_2]\n",
    "\n",
    "def predict(path, neg_probs, pos_probs, neg_prior, pos_prior):\n",
    "    # Calculate the likelihoods of positive and negative\n",
    "    likelihood_neg = likelihood(path, neg_probs)\n",
    "    likelihood_pos = likelihood(path, pos_probs)\n",
    "\n",
    "    # This step calculates the formula P(Ck|x) = P(Ck)*P(x|Ck)\n",
    "    result_neg = neg_prior*likelihood_neg\n",
    "    result_pos = pos_prior*likelihood_pos\n",
    "\n",
    "    # Determine if positive or negative via comparison\n",
    "    if result_pos >= result_neg:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "def accuracy(neg_dir, pos_dir):\n",
    "    # Set the counting variables used while parsing documents\n",
    "    # NOTE: total_classified is incremented every time, as this counts every TP, TN, FP, and FN, becoming our denominator\n",
    "    total_correct = 0\n",
    "    total_classified = 0\n",
    "\n",
    "    # Parse the negative documents\n",
    "    for f in os.listdir(neg_dir):\n",
    "        path = os.path.join(neg_dir, f)\n",
    "        if os.path.isfile(path):\n",
    "            # Get the prediction\n",
    "            p = predict(path, neg_probs, pos_probs, neg_prior, pos_prior)\n",
    "            # If the prediction is 'negative' (as it should be), it is a TN, otherwise it is an FP\n",
    "            if p == 'negative':\n",
    "                total_correct += 1\n",
    "            total_classified += 1\n",
    "\n",
    "    # Parse the positive documents\n",
    "    for f in os.listdir(pos_dir):\n",
    "        path = os.path.join(pos_dir, f)\n",
    "        if os.path.isfile(path):\n",
    "            # Get the prediction\n",
    "            p = predict(path, neg_probs, pos_probs, neg_prior, pos_prior)\n",
    "            # If the prediction is 'positive' (as it should be), it is a TP, otherwise it is an FN\n",
    "            if p == 'positive':\n",
    "                total_correct += 1\n",
    "            total_classified += 1\n",
    "\n",
    "    # total_correct is (TP + TN), total_classified is (TP + TN + FP + FN), so this return calculates the accuracy formula\n",
    "    return (total_correct/total_classified)\n",
    "\n",
    "# Use the class_priors function to get priors (we already have neg_probs and pos_probs from last question)\n",
    "(neg_prior, pos_prior) = class_priors('review_polarity/txt_sentoken/neg', 'review_polarity/txt_sentoken/pos')\n",
    "\n",
    "\n",
    "# Some testcases with correct and incorrect classifications\n",
    "path = 'review_polarity/txt_sentoken/pos/cv996_11592.txt'\n",
    "print(path, 'classified as', predict(path, neg_probs, pos_probs, neg_prior, pos_prior))\n",
    "\n",
    "path = 'review_polarity/txt_sentoken/pos/cv000_29590.txt'\n",
    "print(path, 'classified as', predict(path, neg_probs, pos_probs, neg_prior, pos_prior))\n",
    "\n",
    "path = 'review_polarity/txt_sentoken/neg/cv001_19502.txt'\n",
    "print(path, 'classified as', predict(path, neg_probs, pos_probs, neg_prior, pos_prior))\n",
    "\n",
    "path = 'review_polarity/txt_sentoken/neg/cv000_29416.txt'\n",
    "print(path, 'classified as', predict(path, neg_probs, pos_probs, neg_prior, pos_prior))\n",
    "\n",
    "print(\"Overall Accuracy: \", accuracy('review_polarity/txt_sentoken/neg', 'review_polarity/txt_sentoken/pos') * 100., \"%\")\n",
    "\n",
    "# Expected Output\n",
    "# review_polarity/txt_sentoken/pos/cv996_11592.txt classified as positive\n",
    "# review_polarity/txt_sentoken/pos/cv000_29590.txt classified as negative\n",
    "# review_polarity/txt_sentoken/neg/cv001_19502.txt classified as positive\n",
    "# review_polarity/txt_sentoken/neg/cv000_29416.txt classified as negative\n",
    "# Overall Accuracy:  67.05 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 (Basic)  - Specifying a Bayesian Network - 6 Marks\n",
    "\n",
    "<img src=\"dyspnea.png\">\n",
    "\n",
    "Using the conventions for DBNs used in probability.ipynb (from the AIMA authors) encode the dyspnea network shown above, note we've provided the code for this below. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defines BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration and rejection sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 8019, True: 1981})\n"
     ]
    }
   ],
   "source": [
    "# Example of some sampling of a Bayes Node\n",
    "\n",
    "from collections import Counter \n",
    "bn = BayesNode('X', 'Burglary', {True: 0.2, False: 0.625})\n",
    "\n",
    "bn.p(True, {'Burglary': False, 'Earthquake': True})\n",
    "\n",
    "samples = [] \n",
    "for i in range(0,10000): \n",
    "    samples.append(bn.sample({'Burglary': True, 'Earthquake': True}))\n",
    "print(Counter(samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "False: 0.65, True: 0.35\n"
     ]
    }
   ],
   "source": [
    "# Example of a BayesNet and some queries\n",
    "\n",
    "burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', ['Burglary', 'Earthquake'],\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "print(rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000).show_approx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True,): 0.1, (False,): 0.01}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "# Name your Bayes network for Dispnea  and specify it below \n",
    "dispnea = BayesNet([\n",
    "    ('A', '', 0.001),\n",
    "    ('S', '', 0.5),\n",
    "    ('T', 'A', {True: 0.05, False: 0.01}),\n",
    "    ('L', 'S', {True: 0.1, False: 0.01}),\n",
    "    ('B', 'S', {True: 0.6, False: 0.3}),\n",
    "    ('E', ['T', 'L'],\n",
    "     {(True, True): 1, (True, False): 1, (False, True): 1, (False, False): 0}),\n",
    "    ('X', 'E', {True: 0.98, False: 0.05}),\n",
    "    ('D', ['E', 'B'],\n",
    "     {(True, True): 0.9, (True, False): 0.7, (False, True): 0.8, (False, False): 0.1}),\n",
    "    ])\n",
    "\n",
    "print(dispnea.variable_node('L').cpt)\n",
    "\n",
    "# Expected output\n",
    "# {(True,): 0.1, (False,): 0.01}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Expected) - Querying the Bayesian Network - 6 Marks\n",
    "\n",
    "Answer the following queries using exact inference with enumeration and approximate inference of the same queries using both rejection sampling and likelihood weighting: \n",
    "\n",
    "1. given that a patient has tuberculosis, what is the likelihood of being a smoker?\n",
    "2. given that a patient has been in Asia and has a positive x-ray, what is the likelihood of having dyspnea?\n",
    "3. given that a patient is a smoker and has lung cancer, what is the likelihood of having dyspnea?\n",
    "\n",
    "Use 100000 samples where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood of Smoker given Tuberculosis using exact inference: 0.5\n",
      "Likelihood of Smoker given Tuberculosis using approximate rejection sampling: 0.5029069767441861\n",
      "Likelihood of Smoker given Tuberculosis using approximate likelihood weighting: 0.4999501733965796\n",
      "Likelihood of Dispnea given VisitToAsia and PositiveXRay using exact inference: 0.6811011940658546\n",
      "Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate rejection sampling: 0.4615384615384616\n",
      "Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate likelihood weighting: 0.6847340994075521\n",
      "Likelihood of Dispnea given LungCancer and Smoker using exact inference: 0.8200000000000001\n",
      "Likelihood of Dispnea given LungCancer and Smoker using approximate rejection sampling: 0.8102812803103783\n",
      "Likelihood of Dispnea given LungCancer and Smoker using approximate likelihood weighting:  0.820640000000265\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "# Queries for part 1.\n",
    "query1 = enumeration_ask('S', {'T': True}, dispnea)\n",
    "print(f\"Likelihood of Smoker given Tuberculosis using exact inference: {query1[True]}\")\n",
    "print(f\"Likelihood of Smoker given Tuberculosis using approximate rejection sampling: {rejection_sampling('S', dict(T=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "print(f\"Likelihood of Smoker given Tuberculosis using approximate likelihood weighting: {likelihood_weighting('S', dict(T=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "\n",
    "# Queries for part 2.\n",
    "query2 = enumeration_ask('D', {'A': True, 'X': True}, dispnea)\n",
    "print(f\"Likelihood of Dispnea given VisitToAsia and PositiveXRay using exact inference: {query2[True]}\")\n",
    "print(f\"Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate rejection sampling: {rejection_sampling('D', dict(A=True, X=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "print(f\"Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate likelihood weighting: {likelihood_weighting('D', dict(A=True, X=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "\n",
    "# Queries for part 3.\n",
    "query3 = enumeration_ask('D', {'S': True, 'L': True}, dispnea)\n",
    "print(f\"Likelihood of Dispnea given LungCancer and Smoker using exact inference: {query3[True]}\")\n",
    "print(f\"Likelihood of Dispnea given LungCancer and Smoker using approximate rejection sampling: {rejection_sampling('D', dict(S=True, L=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "print(f\"Likelihood of Dispnea given LungCancer and Smoker using approximate likelihood weighting: {likelihood_weighting('D', dict(S=True, L=True), dispnea, 100000).show_approx(numfmt='{:.16g}')[-18:]}\")\n",
    "\n",
    "# Expected output - note the sampling methods should return similar but not exact numbers\n",
    "# Likelihood of Smoker given Tuberculosis using exact inference:  0.5\n",
    "# Likelihood of Smoker given Tuberculosis using approximate rejection sampling:  0.4874274661508704\n",
    "# Likelihood of Smoker given Tuberculosis using approximate likelihood weighting:  0.49887711620407665\n",
    "# Likelihood of Dispnea given VisitToAsia and PositiveXRay using exact inference:  0.6811011940658546\n",
    "# Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate rejection sampling:  0.727810650887574\n",
    "# Likelihood of Dispnea given VisitToAsia and PositiveXRay using approximate likelihood weighting:  0.6789105866434472\n",
    "# Likelihood of Dispnea given LungCancer and Smoker using exact inference:  0.8200000000000001\n",
    "# Likelihood of Dispnea given LungCancer and Smoker using approximate rejection sampling:  0.8162444712505026\n",
    "# Likelihood of Dispnea given LungCancer and Smoker using approximate likelihood weighting:  0.8185400000002676\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
