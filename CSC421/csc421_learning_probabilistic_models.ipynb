{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 20 - Learning Probabilistic Models 1\n",
    "\n",
    "### Instructor: Brandon Haworth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Credit: George Tzanetakis\n",
    "Jupyter Notebooks you encounter during the course were largely developed by Prof. Tzanetakis from a previous iteration of this course. I've since changed/developed them where necessary for my own iterations of CSC 421."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example, the Expected reading includes the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **20.1**, **20.2.1**, **20.2.2**, and **Summary**\n",
    "2. Expected: Same as Basic plus **20.3**, **20.3.1**, **20.3.3**\n",
    "3. Advanced: All the chapter including bibliographical and historical notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered a variety of probabilistic models that model uncertainty and allow us to do inference in different ways. In this notebook, we describe some of the ways we can estimate probabilistic models from data. \n",
    "These techniques provide the connection between statistics, probability, and machine learning. \n",
    "\n",
    "The ideas are based on Chapter 20 of the Artificial Intelligence: a Modern Approach textbook and specifically Section 20.2 Learning from Complete Data. \n",
    "\n",
    "**Density estimation** refers to the task of learning the probability distribution function a probability density function (for continuous models) or the probability mass function (for discrete models) given some data that we assume was generated from that model. **Complete data** means that we have data for all the **variables** in our model. \n",
    "\n",
    "The most common type of learning is **parameter learning** where we assume a particular structure for our model and characterize it by estimating a set of parameters. For example, we might assume a normal or Gaussian multi-variate distribution and estimate the mean vector and the covariance matrix that characterizes it. As another example, we might be given the structure of a Bayesian network (in terms of parent/child conditional relationships) and learn the conditional probability tables. We will also briefly discuss the problem of learning structure as well as non-parametric density estimation in which we don't need to make any assumptions about the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Complete Data \n",
    "\n",
    "## A random variable class \n",
    "\n",
    "Define a helper random variable class based on the scipy discrete random variable functionality providing both numeric and symbolic RVs. You don't need to look at the implementation - the usage will be obvious through the examples below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np \n",
    "\n",
    "class Random_Variable: \n",
    "    \n",
    "    def __init__(self, name, values, probability_distribution): \n",
    "        self.name = name \n",
    "        self.values = values \n",
    "        self.probability_distribution = probability_distribution \n",
    "        if all(type(item) is np.int64 for item in self.values): \n",
    "            self.type = 'numeric'\n",
    "            self.rv = stats.rv_discrete(name = name, \n",
    "                        values = (values, probability_distribution))\n",
    "        elif all(type(item) is str for item in values): \n",
    "            self.type = 'symbolic'\n",
    "            self.rv = stats.rv_discrete(name = name, \n",
    "                        values = (np.arange(len(values)), probability_distribution))\n",
    "            self.symbolic_values = values \n",
    "        else: \n",
    "            self.type = 'undefined'\n",
    "            \n",
    "    def sample(self,size): \n",
    "        if (self.type =='numeric'): \n",
    "            return self.rv.rvs(size=size)\n",
    "        elif (self.type == 'symbolic'): \n",
    "            numeric_samples = self.rv.rvs(size=size)\n",
    "            mapped_samples = [self.values[x] for x in numeric_samples]\n",
    "            return mapped_samples \n",
    "        \n",
    "    def prob_of_value(self, value): \n",
    "        indices = np.where(self.values == value)\n",
    "        return self.probability_distribution[indices[0][0]]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood of model given some data \n",
    "\n",
    "First, let's review the concept of the likelihood of a model given some data \n",
    "\n",
    "\n",
    "Let's start by creating a random variable corresponding to a 6-faced dice where there are two faces with the numbers 1,2 and 3 therefore each number appears with equal probability. We can generate random samples from this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 2 2 2 3 3 3 2 3 2 2 2 1 1 3 2 2 1 2 1 3 3 2 3 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "values = np.int64([1, 2, 3])\n",
    "probabilities = [2/6., 2/6., 2/6.]\n",
    "dice1 = Random_Variable('dice1', values, probabilities)\n",
    "samples = dice1.sample(30)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a random variable where two faces have the number 1, three faces have the number 2, and one face has the number 3. We can also generate random samples from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 3 2 2 2 1 3 3 2 1 2 1 3 1 2 1 1 1 2 2 2 2 3 2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "values = np.int64([1, 2, 3])\n",
    "probabilities = [2./6, 3./6, 1./6]\n",
    "dice2 = Random_Variable('dice2', values, probabilities)\n",
    "samples = dice2.sample(30)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of a sequence of samples given a model can be obtained by taking the product of the corresponding \n",
    "probabilities. We can see that for this particular sequence of data, the likelihood of the model for dice2 is higher. So if we have some data and some specific models we can select the model with the highest likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of each value in the R.V. for Dice 1\n",
      "Face with 1: 0.333333\n",
      "Face with 2: 0.333333\n",
      "Face with 3: 0.333333\n",
      "Probability of each value in the R.V. for Dice 2\n",
      "Face with 1: 0.333333\n",
      "Face with 2: 0.500000\n",
      "Face with 3: 0.166667\n",
      "\n",
      "Assume we have the following observed data:\n",
      "[1, 2, 2, 1, 1, 3, 1, 2, 3, 2]\n",
      "Likelihood for Dice 1: 0.000017\n",
      "Likelihood for Dice 2: 0.000021\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,2,1,1,3,1,2,3,2]\n",
    "print(\"Probability of each value in the R.V. for Dice 1\")\n",
    "print(\"Face with 1: %f\" % dice1.prob_of_value(1))\n",
    "print(\"Face with 2: %f\" % dice1.prob_of_value(2))\n",
    "print(\"Face with 3: %f\" % dice1.prob_of_value(3))\n",
    "print(\"Probability of each value in the R.V. for Dice 2\")\n",
    "print(\"Face with 1: %f\" % dice2.prob_of_value(1))\n",
    "print(\"Face with 2: %f\" % dice2.prob_of_value(2))\n",
    "print(\"Face with 3: %f\" % dice2.prob_of_value(3))\n",
    "\n",
    "def likelihood(data, model):\n",
    "    likelihood = 1.0 \n",
    "    for d in data: \n",
    "        likelihood *= model.prob_of_value(d)\n",
    "    return likelihood \n",
    "\n",
    "print(\"\\nAssume we have the following observed data:\")\n",
    "print(data)\n",
    "print(\"Likelihood for Dice 1: %f\" % likelihood(data,dice1))\n",
    "print(\"Likelihood for Dice 2: %f\" % likelihood(data,dice2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even with only 10 values the likelihood gets relatively small and we can expect it will get smaller as the sequences of data get smaller. We can also use log-likelihood to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.16666666666666666\n",
      "\n",
      "Assume we have the following observed data:\n",
      "[1, 2, 2, 1, 1, 3, 1, 2, 3, 2]\n",
      "Likelihood for Dice 1: -10.986123\n",
      "Likelihood for Dice 2: -10.750557\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,2,1,1,3,1,2,3,2]\n",
    "print(dice1.prob_of_value(1))\n",
    "print(dice1.prob_of_value(3))\n",
    "print(dice2.prob_of_value(3))\n",
    "\n",
    "def log_likelihood(data, model):\n",
    "    likelihood = 0.0 \n",
    "    for d in data: \n",
    "        likelihood += np.log(model.prob_of_value(d)) # Note the change to summation!\n",
    "    return likelihood \n",
    "    \n",
    "print(\"\\nAssume we have the following observed data:\")\n",
    "print(data)\n",
    "print(\"Likelihood for Dice 1: %f\" % log_likelihood(data,dice1))\n",
    "print(\"Likelihood for Dice 2: %f\" % log_likelihood(data,dice2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above we examined two possible models. One could ask the question of all possible models for a particular problem, can we find the one with the highest likelihood? If we have a dice with six faces that can only have the numbers 1, 2, and 3 then there is a finite amount of models and we can calculate their likelihoods as we did above. However, if we relax the requirement to have a dice and simply have the values 1,2 and 3 but with arbitrarily associated probabilities then we have an infinite number of possible models. Without going into the math it turns out that at least for this particular case the model that will have the maximum likelihood can be simply obtained by counting the relative frequencies of the values in the data. This is called maximum likelihood estimation of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 7, 1: 4, 3: 2})\n",
      "[0.3076923076923077, 0.5384615384615384, 0.15384615384615385]\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "data = [1,2,2,1,1,3,1,2,3,2,2,2,2]\n",
    "counts = collections.Counter(data)\n",
    "print(counts)\n",
    "est_probability_distribution = [counts[1]/float(len(data)), counts[2]/float(len(data)), counts[3]/float(len(data))]\n",
    "print(est_probability_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new R.V. using the estimated probability distribution from the frequency of the observed samples. This should look like it comes from the same source/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 2 3 2 2 2 2 2 2 2 1 1 2 1 1 1 2 2 2 1 2 2 2 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "values = np.int64([1, 2, 3])\n",
    "probabilities = est_probability_distribution \n",
    "model = Random_Variable('model', values, probabilities)\n",
    "samples = model.sample(30)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-likelihood parameter learning for Discrete Models  \n",
    "\n",
    "\n",
    "Let's start by creating a random variable corresponding to a bag of candy with two types lime and cherry. We can easily generate random samples from this model. For example, in the code below we generate 100 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'c', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'c', 'c', 'c', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'l', 'l', 'c', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'c', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'c', 'c', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'c', 'c', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'c', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l']\n"
     ]
    }
   ],
   "source": [
    "values = ['c', 'l']\n",
    "probabilities = [0.2, 0.8]\n",
    "dice1 = Random_Variable('bag1', values, probabilities)\n",
    "samples = dice1.sample(1000)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that you are just given these samples and you are told that they were from a bag of candy but you don't know the percentage of each candy type in the bag and you need to estimate it. Let's call the probability *a candy from the bag is cherry* $\\theta$. Then our task of parameter learning is to estimate $\\theta$ from the provided samples. In the previous notebook without much explanation, I stated that the \"best\" possible model in a maximum likelihood sense can be easily obtained by simply counting the percentage of each candy type in our bag. \n",
    "As you can see the estimated parameter $\\theta$ is close but not the same as the original value which was $0.2$. If we had more samples this estimate becomes more accurate.  \n",
    "\n",
    "We can see that with this simple example, we have the ability to \"learn\" a model. Once we have a \"learned\" model from the data we can use it to make predictions or inferences in general as well as generate samples if needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'l': 795, 'c': 205})\n",
      "[0.205, 0.795]\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "counts = collections.Counter(samples)\n",
    "print(counts)\n",
    "est_probability_distribution = [counts['c']/float(len(samples)), counts['l']/float(len(samples))]\n",
    "print(est_probability_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the counts seems intuitive and I told you that for the case of discrete random variables, this provides the maximum likelihood estimate but can we prove this assertion? \n",
    "\n",
    "Here is how we can do it. Each time we have a candy of a particular type we multiply the associated probability to get the likelihood of the sequence (assuming i.i.d. samples). If there are $c$ cherry candies and $l=N-c$ limes then we can write the likelihood as follows: \n",
    "\n",
    "$$ P({\\bf d} | h_{\\theta}) = \\prod_{j=1}^{N} P(d_j | h_{\\theta}) = \\theta^{c} * (1-\\theta)^{l}$$\n",
    "\n",
    "Note: check how the mathematical expression above is notated. It uses LaTeX notation which can be embedded in markdown cells. It is a useful thing to learn to produce nice-looking equations in both notebooks and papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum-likelihood hypothesis is given by the value of $\\theta$ that maximizes the expression above. The same value can be obtained by maximizing the **log-likelihood**. Note that we have used log-likelihood before to avoid small numerical likelihood values when computing over long sequences. Here we use it because it allows us to simplify our expression to prove our approach to maximum likelihood parameter estimation. By taking the log we convert the product to a sum which is easier to maximize. \n",
    "\n",
    "$$ L({\\bf d}| h_{\\theta}) = \\log{P({\\bf d} | h_{\\theta})} = \\sum_{j=1}^{N}\\log{P(d_j| h_{\\theta})} = c \\log(\\theta) + l \\log{(1-\\theta)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maximum-likelihood value of $\\theta$, we differentiate the $L$ with respect to $\\theta$ and set the resulting expression to zero: \n",
    "\n",
    "$$ \n",
    "\\frac{L({\\bf d} | h_{\\theta})}{d \\theta} = \\frac{c}{\\theta} - \\frac{l}{1-\\theta} = 0 \n",
    "$$\n",
    "\n",
    "Solving for $\\theta$ we get: \n",
    "$$ \n",
    "\\theta = \\frac{c}{c+l} = \\frac{c}{N}\n",
    "$$ \n",
    "\n",
    "This might seem like a lot of work to prove something obvious but now we actually know that of all the infinite possible models of bags we could have - the one we estimate by counting the proportion of candy is the \"best\" in a maximum likelihood sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we followed can be used for a variety of probabilistic models. The steps are as follows: \n",
    "\n",
    "1. Write down an expression for the likelihood of the data as a function of the parameters and use log to simplify it for step 2 \n",
    "2. Write down the derivative of the log-likelihood with respect to each parameter \n",
    "3. Find the parameter values such that the derivatives are zero. \n",
    "\n",
    "Note: If we are lucky we are able to perform steps 2 and 3 analytically and derive an exact ML parameter estimate. There are many cases especially when dealing with continuous models (which we cover below) in which maximizing the likelihood function analytically is not possible and one needs to resort to numerical methods \n",
    "which do not provide an exact solution. \n",
    "\n",
    "As another example of analytical ML parameter estimation, the book has one more example in which there is an extra random variable wrapper and the model has three parameters $\\theta_1, \\theta_2, \\theta_3$. \n",
    "\n",
    "For example, by basically filtering the data and counting we can do ML parameter estimation for Naive Bayes models as well as Bayesian Networks with discrete random variables. You have already seen to some extent how this can be done during lectures as well as in the assignments. Notice that the structure of the Bayesian network allows us to simplify the problem of ML parameter estimation by factoring different groups of variables based on their conditional structural relationships. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-likelihood parameter learning for continuous models \n",
    "\n",
    "\n",
    "Continuous probability models are heavily used in real-world applications. In many cases, we need to resort to numerical optimization methods to perform parameter estimation. However, in some cases, we can get the exact answer analytically. Let's consider the simple example of learning the parameters of a Gaussian density function on a single variable. Similarly to what we did in the previous section for learning the parameters of a discrete random variable, we will first generate some data and then estimate the parameters from the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be generated using a Gaussian density function on a single variable. The corresponding equation is (specifically for numpy.random.normal): \n",
    "\n",
    "$$ P(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "The parameters of this model are the mean $\\mu$ and the standard deviation $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.72624835 3.05506211 2.85027426 3.12181333 3.14033344 3.1204349\n",
      " 2.95032936 2.61363656 2.98996048 3.20722797]\n"
     ]
    }
   ],
   "source": [
    "mu = 3.0 \n",
    "sigma = 0.2 \n",
    "s = np.random.normal(mu, sigma, 10)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.18787276 2.83407902 2.84811335 ... 3.30848375 3.01161803 2.99270717]\n"
     ]
    }
   ],
   "source": [
    "samples = np.random.normal(mu, sigma, 10000)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the observed values by $x_1, \\dots, x_N$. Then the log-likelihood is: \n",
    "\n",
    "$$ \n",
    "L = \\sum_{j=1}^{N} \\log \\left({\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}\\right) = -\\frac{N}{2}\\log(2\\pi\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{j=1}^{N} (x_j-\\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivatives to zero we obtain: \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial \\mu} = - \\frac{1}{\\sigma^2}\\sum_{j=1}^{N} (x_j-\\mu) = 0 \n",
    "$$\n",
    "which implies: \n",
    "$$ \n",
    "\\mu = \\frac{\\sum_j x_j}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the maximum likelihood value of the mean is the sample average. Similarly, you can find that the maximum likelihood value of the standard deviation is the square root of the sample variance. You can check the textbook for the details of the standard deviation $\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how we can calculate these ML parameter estimates for the data that we have. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.999387962350083\n",
      "2.999387962350083\n",
      "0.19982809234770463\n"
     ]
    }
   ],
   "source": [
    "estimated_mean1 = np.sum(samples) / len(samples)\n",
    "print(estimated_mean1)\n",
    "estimated_mean2 = np.mean(samples)\n",
    "print(estimated_mean2)\n",
    "estimated_std = np.std(samples)\n",
    "print(estimated_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So armed with simple filtering, counting and calculating sample mean and sample standard deviation we have everything we need to estimate the probabilities of a Naive Bayes model that contains a mixture of continuous and discrete variables. For the discrete variables, we count and estimate directly the probabilities. For the continuous variables, we first estimate the ML parameters (sample mean and standard deviation) and then for a particular value of the feature we use the single variable Gaussian density equation to derive a probability value for that value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes for further reading - not needed for the \"final\" assignment for those interested in digging deeper. \n",
    "\n",
    "**Note1**: In a Bayesian network with continuous variables you have the problem of having a continuous parent and a continuous child variable. These can be addressed with linear Gaussian models. More details in the textbook \n",
    "\n",
    "**Note2**: Similarly to Bayesian learning in discrete models one can follow a similar approach and use a hypothesis prior to guiding the learning. The textbook shows an example that uses **beta distributions** you can check out. \n",
    "\n",
    "**Note3**: If you remember when we covered the Bayesian network we looked at approximate inference using direct sampling and rejection sampling. You will notice that the approach we followed was similar to statistical learning in the sense that we generated samples and then used counting to estimate probabilities. So at a basic level inference and learning can be considered the same process. We start with a few things that we know and then using data we update what we know. \n",
    "\n",
    "**Note4**: It is also possible to learn the structure of a Bayesian network from data. The basic idea is to search over the space of possible models. To do so we will need some method to determine when a good structure has been found. More details can be found in the book. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
