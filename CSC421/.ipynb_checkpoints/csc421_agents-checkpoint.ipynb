{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSC 421 - Agents \n",
    "\n",
    "### Instructor: Brandon Haworth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Credit: George Tzanetakis\n",
    "Jupyter Notebooks you encounter during the course were largely developed by Prof. Tzanetakis from a previous iteration of this course. I've since changed/developed them where necessary for my own iterations of CSC 421."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents \n",
    "\n",
    "\n",
    "**EMPHASIS**: Agents as a unifying concept of thinking about AI and software \n",
    "\n",
    "\n",
    "During this lecture, we will cover the following topics: \n",
    "\n",
    "1. Agents \n",
    "2. Performance, environments, actuators, sensors \n",
    "3. Agent architectures \n",
    "7. Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example, the Expected reading includes the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **2.1**, **2.3**, **2.4** and Summary  \n",
    "2. Expected: **2.2**\n",
    "3. Advanced: Bibliographical and historical notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents and Environments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents** perceive their **environment** through **sensors** and act upon that environment through **actuators**. \n",
    "\n",
    "\n",
    "Terminology: \n",
    "\n",
    "1. Percept \n",
    "2. Percept sequence \n",
    "3. Agent function (abstract mathematical distribution, in many cases infinite tabulation) \n",
    "4. Agent program (concrete implementation running on a physical system) \n",
    "\n",
    "What makes an agent effective, good, intelligent? \n",
    "\n",
    "\n",
    "Any area of engineering can be viewed through the lenses of agents. What makes AI unique is the significant computational resources that can be employed by the agent and the non-trivial decision-making that the task environment requires. \n",
    "\n",
    "<img src=\"images/aima_simple_agent.png\" width=\"60%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization of the agent paradigm\n",
    "\n",
    "The generalization of this paradigm could be taken to almost all areas, eventually trivializing the utility of the idea.\n",
    "\n",
    "In fact, one could define AI agents as systems that can not be developed using traditional engineering approaches.\n",
    "\n",
    "**Imagine for example a simple function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n",
      "Zero\n"
     ]
    }
   ],
   "source": [
    "def agent_function(percept):\n",
    "    action = \"Zero\"\n",
    "    if percept > 0:\n",
    "        action = \"Positive\"\n",
    "    elif percept < 0:\n",
    "        action = \"Negative\"\n",
    "    return action\n",
    "\n",
    "def agent(percept):\n",
    "    action = agent_function(percept)\n",
    "    print(action)\n",
    "\n",
    "agent(-1)\n",
    "agent(1)\n",
    "agent(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that same function written for some program you need to identify the sign of a number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n",
      "Zero\n"
     ]
    }
   ],
   "source": [
    "def print_sign(number):\n",
    "    result = \"Zero\"\n",
    "    if number > 0:\n",
    "        result = \"Positive\"\n",
    "    elif number < 0:\n",
    "        result = \"Negative\"\n",
    "    print(result)\n",
    "\n",
    "print_sign(-1)\n",
    "print_sign(1)\n",
    "print_sign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The utility of this description is in understanding AI systems. It is not meant to remap everything to AI and say all computing things are AI.**\n",
    "\n",
    "**AI, for our purposes, is something that has significant computational resources and its task environment requires non-trivial decision-making**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEAS Description of Agent \n",
    "\n",
    "1. **Performance** a way to measure how the agent is doing \n",
    "2. **Environment** essential to the problems or worlds in which the agent needs to operate \n",
    "3. **Actuators** are the different ways the agent can interact with the environment as well as possibly its own\n",
    "operation. They receive **actions** that encode the information needed for them to operate. \n",
    "4. **Sensors** are the ways the agent can acquire information about the environment it is operating as well as possibly \n",
    "information about its own operation. The information they acquire is represented as **percepts**.\n",
    "\n",
    "Let's consider some examples - what are the possible percepts, environments, sensors, actuators, and actions for these \n",
    "agents: \n",
    "\n",
    "1.  Human \n",
    "2.  Robot \n",
    "3.  Vacuum cleaner world \n",
    "4.  Single chess piece valid chessboard moves \n",
    "5.  Self-driving car \n",
    "6.  Ant \n",
    "7.  NPC in-game \n",
    "8.  Chess playing program \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance and Rationality\n",
    "In some ways, the concept of a performance measure ties the PEAS concept together in the framework of this course.\n",
    "\n",
    "The book has an excellent definition of a **rational agent**, that centres rationality around performance.\n",
    "> For each possible **percept sequence**, a **rational agent** should select an **action** that is expected to maximize its **performance** measure, given the evidence provided by the **percept sequence** and whatever built-in knowledge the agent has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK ENVIRONMENTS \n",
    "\n",
    "Specifying the task environment (essentially the problem to which rational agents are the solutions): \n",
    "\n",
    "\n",
    "1. Performance \n",
    "2. Environment \n",
    "3. Actuators \n",
    "4. Sensors \n",
    "\n",
    "\n",
    "Properties of task environments (for each one think of examples or consider the examples mentioned above): \n",
    "\n",
    "1. Fully observable vs partially observable\n",
    "2. Single-agent vs multiagent \n",
    "    1. Competitive multiagent (chess) vs cooperative multiagent (self-driving cars avoiding collisions)\n",
    "3. Deterministic vs nondeterministic\n",
    "\n",
    "**Agent = architecture + program** \n",
    "An AI agent is its agent program and the architecture it runs on, a computer, a robot chassis, etc, where for example:\n",
    "1. computational power\n",
    "2. storage media and their capacity\n",
    "3. sensors and their fidelity\n",
    "4. actuators and their degrees of freedom and physical capabilities,\n",
    "dictate the operational limits of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully observable vs partially observable\n",
    "**Fully observable** environments mean that the entire state of the environment is available to the sensors of the agent. In particular, the environmental information that is *relevant*.\n",
    "\n",
    "**Partial observability** of a task environment introduces unique challenges. The agent may observe what is available to its sensors but depending on the internal construction of the agent may struggle to reason within the environment.\n",
    "\n",
    "**There are many reasons an environment may be only partially observable**\n",
    "\n",
    "Often in real life, outside heavily controlled environments, environments are partially observable. This makes day-to-day AI agents difficult to develop. Consider the autonomous vehicle..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-agent vs multiagent\n",
    "The **single-agent** task environment allows us to focus on the interactions of that agent with the task environment\n",
    "\n",
    "The **multi-agent** task environment raises interesting questions and issues in our agents' operations. \n",
    "\n",
    "Consider \n",
    "1. Are other things in the environment agents?\n",
    "    1. What makes an agent to an agent?\n",
    "    2. When must something be considered an agent by another agent?\n",
    "2. Do those agents communicate?\n",
    "3. What is the interaction between the concepts of partial observability and multi-agent environments? (HINT: mind reading)\n",
    "    1. Are those other agents now part of the environment w.r.t the agent?\n",
    "4. Are these agents competitive or cooperative?\n",
    "    1. Can we engage game theory to understand maximizing their performance concerning each other?\n",
    "    2. Co-operative agents in game theory are viewed as a group for utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic vs nondeterministic\n",
    "**Deterministic environments** are those environments where the next state of the environment is entirely dependent on the current state of the environment and the action the agent takes. In this way, given our transition function, we can say exactly what state we will end up in.\n",
    "\n",
    "**Non-deterministic environments** raise several interesting issues and questions about how our agent operates.\n",
    "\n",
    "Consider \n",
    "\n",
    "1. Do we know what is even possible in this environment? That is, even though we may not know the next state with certainty, can we enumerate all possible states?\n",
    "2. Are real-life states/environments non-deterministic? Does this complicate agent designs? (HINT: human behaviour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Agents \n",
    "\n",
    "1. **Reflex agents** \n",
    "\n",
    "Reflex agents simply act based on the current percept ignoring the rest of the percept history. They can \n",
    "operate using simple condition-action rules. Humans have many such rules that are typically used when fast action \n",
    "in response to a stimulus is required. \n",
    "\n",
    "<img src=\"images/aima_simple_agent.png\" width=\"60%\"/>\n",
    "\n",
    "2. **Model-based reflex agents**\n",
    "\n",
    "The most effective way for an agent to deal with a partially observable environment is to maintain \n",
    "some kind of internal representation (model) keeping track of the parts of the world that it can not \n",
    "perceive. This model needs to be updated based on knowledge about how the world changes independently of the \n",
    "agent as well as about how the agent's own actions can affect the world. \n",
    "\n",
    "<img src=\"images/aima_model_agent.png\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "\n",
    "5. **Goal-based agents** \n",
    "\n",
    "Knowing something about the current state of the world is not always sufficient in order to decide what to do. There are many situations in which the agent needs to have some sort of **goal** information that describes situations that are desirable. Goal-based agents are fundamentally different than reflex agents as they consider the future. We will be looking at **Search** (in detail) and **Planning** which are two research areas of AI that focus on finding action sequences for agents to achieve specific goals. Goal agents are more flexible than reflex-agents but in general, tend to be more computationally demanding and therefore slower as they need to consider how actions create multiple possible \"futures\" and determine if these **futures** meet specific **goals**. \n",
    "\n",
    "<img src=\"images/aima_goal_agent.png\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "\n",
    "6. **Utility-based agents**\n",
    "   \n",
    "Goals alone are not enough. Utility is an internal representation of the performance measure. You can think of it as a \"happiness\" measure for the agent. Utility has the same relationship with an external performance measure that the internal world representation of an agent has with the actual world/environment it operates. Utility can assist in two situations in which goal-based agents have a hard time: 1) when there are conflicting goals (for example speed and safety) and 2) when there are multiple goals that the agent can aim for but none of which can achieved with certainty. Because uncertainty is always present in typical real-world situations requiring rational/intelligent behaviour technically speaking a utility-based agent chooses the action that maximizes the **expected utility**. \n",
    "\n",
    "<img src=\"images/aima_utility_agent.png\" width=\"60%\"/>\n",
    "\n",
    "Discussion Examples: \n",
    "NPC in a graph-based text adventure \n",
    "Driving to the airport \n",
    "\n",
    "\n",
    "\n",
    "### LEARNING AGENTS \n",
    "\n",
    "All types of agent architectures can benefit from learning. Learning occurs when the performance measure with which we measure how the agent is doing improves through \"experience\" i.e repeated operation in an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World/Environment Representations \n",
    "\n",
    "**Important note:** The environment representation is different than the environment and it consists of what \n",
    "the agent \"knows\" about it so in most cases it does not contain all the information in the environment. \n",
    "\n",
    "Atomic representation, factored representation, structured representation, distributed representation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/aima_environment_representations.png\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "* Algorithms for search and game-playing, hidden markov modes, and markov decision processes work with atomic representations. \n",
    "* Constraint satisfaction problems, propositional logic, bayesian networks, and machine learning algorithms frequently work with factored representatinos. \n",
    "* Relational databases, first-order logic, natural language understanding and knowledge-based learning operate on structured representation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Agent Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code sketch of a reflex agent. The set of rules (condition-action pairs) \n",
    "# can remain static through the execution or can be modified if the agent is capable of learning. \n",
    "def simple_rule_agent(percept, rules): \n",
    "    state = interpet_input(percept)\n",
    "    rule  = rule_match(state,rules) \n",
    "    action = rule.action() \n",
    "    return action\n",
    "\n",
    "\n",
    "# code sketch of a model-based reflex agent \n",
    "# state: the agent's current conception of the world state \n",
    "# model: a description of how the next state depends on the current state and action \n",
    "def model_agent(percept, rules, state, model): \n",
    "    state = update_state(state, action, percept, model) \n",
    "    rule = rule_match(state, rules) \n",
    "    action = rule.action() \n",
    "    return action \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
